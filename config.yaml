app:
  name: Zenith
  version: 0.1.0
  environment: development
  debug: true

server:
  host: 0.0.0.0
  port: 8000
  workers: 4
  reload: true

database:
  url: ${DATABASE_URL}
  pool_size: 20
  max_overflow: 40
  pool_pre_ping: true
  echo: false

redis:
  url: ${REDIS_URL}
  max_connections: 50
  decode_responses: true

celery:
  broker_url: ${CELERY_BROKER_URL}
  result_backend: ${CELERY_RESULT_BACKEND}
  task_serializer: json
  result_serializer: json
  accept_content:
    - json
  timezone: UTC
  enable_utc: true

security:
  secret_key: ${SECRET_KEY}
  algorithm: HS256
  access_token_expire_minutes: 30
  refresh_token_expire_days: 7

storage:
  type: s3
  bucket: zenith-artifacts
  endpoint: ${S3_ENDPOINT}
  access_key: ${S3_ACCESS_KEY}
  secret_key: ${S3_SECRET_KEY}
  region: us-east-1

feature_store:
  online:
    backend: redis
    ttl_seconds: 86400
  offline:
    backend: parquet
    path: /data/features

inference:
  triton:
    url: http://triton:8000
    timeout: 30
  vllm:
    enabled: false
    url: http://vllm:8000

observability:
  opentelemetry:
    enabled: true
    exporter: otlp
    endpoint: ${OTEL_EXPORTER_OTLP_ENDPOINT}
    service_name: zenith-backend
  prometheus:
    enabled: true
    port: 9090

monitoring:
  drift_detection:
    enabled: true
    check_interval_minutes: 60
    threshold: 0.05
  performance:
    enabled: true
    alert_latency_ms: 1000

llm:
  default_provider: openai
  providers:
    openai:
      api_key: ${OPENAI_API_KEY}
      model: gpt-4-turbo-preview
    anthropic:
      api_key: ${ANTHROPIC_API_KEY}
      model: claude-3-opus-20240229

training:
  distributed:
    backend: nccl
    world_size: 1
  mixed_precision: fp16
  gradient_checkpointing: true

logging:
  level: INFO
  format: json
  handlers:
    - console
    - file
  file:
    path: /var/log/zenith/app.log
    max_bytes: 104857600
    backup_count: 10
